**Abstract**
- T·∫≠p trung gi·ªõi thi·ªáu m√¥ h√¨nh m·ªõi: Transformer
- Gi·ªõi thi·ªáu k·∫øt qu·∫£ ƒëi·ªÉm BLEU tr√™n 2 m√¥ h√¨nh d·ªãch m√°y √°p d·ª•ng ki·∫øn tr√∫c k·ªÉ tr√™n: 28.4 cho d·ªãch t·ª´ Anh sang ƒê·ª©c trong b√†i thi WMT 2014; 41.8 cho b√†i d·ªãch t·ª´ Anh sang Ph√°p sau khi treo 8 CPUs ch·∫°y 3.5 ng√†y. Hu·∫•n luy·ªán cho ra k·∫øt qu·∫£ t·ªët h∆°n k·ªÉ c·∫£ v·ªõi b·ªô d·ªØ li·ªáu l·ªõn v√† h·∫°n ch·∫ø.
1. Introduction
-C√°c c·∫•u tr√∫c thu·∫≠t to√°n x·ª≠ l√Ω c≈©: RNN, LSTM, GRNN
-> H·∫°n ch·∫ø: Gi·ªõi h·∫°n c∆° b·∫£n c·ªßa thu·∫≠t to√°n t√≠nh to√°n tu·∫ßn t·ª± v·∫´n t·ªìn t·∫°i, sau khi t√≠nh to√°n ƒë·ªÉ cho ra ƒë∆∞·ª£c gi√° tr·ªã h_t, khi thu·∫≠t to√°n t√≠nh to√°n t·ªõi ƒë∆∞·ª£c m·ªôt th·ª© t·ª± nh·∫•t ƒë·ªãnh, c√°c d·ªØ li·ªáu c≈© c·ªßa m√¥ h√¨nh training b·ªã "qu√™n" ƒëi
-C∆° ch·∫ø Attention ƒë√£ ƒë∆∞·ª£c ph√°t tri·ªÉn ƒë·ªÉ kh·∫Øc ph·ª•c nh∆∞·ª£c ƒëi·ªÉm n√†y: ch·ªß y·∫øu th·ª±c hi·ªán tr√™n RNN
-ƒê·ªÅ xu·∫•t Transformer, m√¥ h√¨nh ph√π thu·ªôc ho√†n to√†n v√†o c∆° ch·∫ø attention.

2. Background
-M√¥ h√¨nh sequence modeling v√† transduction truy·ªÅn th·ªëng d·ª±a v√†o RNN/CNN ph·ª©c t·∫°p bao g·ªìm encoder-decoder
-C√°c m√¥ h√¨nh t·ªët nh·∫•t ƒë·ªÅu s·ª≠ d·ª•ng c∆° ch·∫ø attention k·∫øt n·ªëi encoder v√† decoder
-Extended Neural GPU, ByteNet, ConvS2S s·ª≠ d·ª•ng CNN l√†m kh·ªëi x√¢y d·ª±ng c∆° b·∫£n, t√≠nh to√°n song song nh∆∞ng s·ªë b∆∞·ªõc ƒë·ªÉ h·ªçc ph·ª• thu·ªôc xa tƒÉng theo kho·∫£ng c√°ch (linearly cho ConvS2S, logarithmically cho ByteNet)
-Transformer gi·∫£m xu·ªëng c√≤n s·ªë b∆∞·ªõc constant, m·∫∑c d√π gi·∫£m ƒë·ªô ph√¢n gi·∫£i hi·ªáu qu·∫£ do averaging attention-weighted positions (ƒë∆∞·ª£c b√π ƒë·∫Øp b·ªüi Multi-Head Attention)
-Self-attention (intra-attention): c∆° ch·∫ø attention li√™n k·∫øt c√°c v·ªã tr√≠ kh√°c nhau c·ªßa m·ªôt sequence ƒë∆°n ƒë·ªÉ t√≠nh to√°n representation c·ªßa sequence ƒë√≥
-Transformer l√† m√¥ h√¨nh transduction ƒë·∫ßu ti√™n ch·ªâ d·ª±a ho√†n to√†n v√†o self-attention, kh√¥ng d√πng RNN/CNN

3. Model Architecture

|-3.1. Encoder and Decoder Stacks
-Encoder: Stack 6 layers gi·ªëng h·ªát nhau
  + M·ªói layer c√≥ 2 sub-layers: 
    * Multi-head self-attention mechanism
    * Position-wise fully connected feed-forward network
  + S·ª≠ d·ª•ng residual connection quanh m·ªói sub-layer, sau ƒë√≥ layer normalization
  + Output c·ªßa m·ªói sub-layer: LayerNorm(x + Sublayer(x))
  + T·∫•t c·∫£ sub-layers v√† embedding layers t·∫°o output dimension d_model = 512

-Decoder: Stack 6 layers gi·ªëng h·ªát nhau
  + M·ªói layer c√≥ 3 sub-layers (th√™m 1 sub-layer so v·ªõi encoder):
    * Masked multi-head self-attention (sub-layer 1)
    * Multi-head attention nh·∫≠n output t·ª´ encoder (sub-layer 2) 
    * Position-wise feed-forward network (sub-layer 3)
  + C≈©ng s·ª≠ d·ª•ng residual connection v√† layer normalization
  + Masking trong self-attention ƒë·∫£m b·∫£o predictions cho v·ªã tr√≠ i ch·ªâ ph·ª• thu·ªôc v√†o outputs ƒë√£ bi·∫øt t·∫°i c√°c v·ªã tr√≠ < i (ngƒÉn ch·∫∑n leftward information flow)

|-3.2. Attention

    |-3.2.1. Scaled Dot-Product Attention
    Embedding l√† g√¨?
    -> L√† c√°c gi√° tr·ªã vector ƒëa chi·ªÅu c·ªßa 1 token ƒë·ªëi v·ªõi c√°c token kh√°c (Nh√∫ng t·ª´ "T·ª´" --> "Vector")
    Dot-Product ·ªü ƒë√¢y l√† g√¨?
    -> Dot-Product ch·ªâ c√°c k·∫øt qu·∫£ nh√¢n t·ª´ c√°c tr·ªçng s·ªë sau khi embedding v·ªõi vector ng·ªØ c·∫£nh h c·ªßa m·ªói tokens
    Query l√† g√¨?
    -> Vector 
    -Input g·ªìm: queries, keys c√≥ dimension d_k, v√† values c√≥ dimension d_v
    -C√¥ng th·ª©c: Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V
    -Gi·∫£i th√≠ch:
      + T√≠nh dot products c·ªßa query v·ªõi t·∫•t c·∫£ keys
      + Chia m·ªói k·∫øt qu·∫£ cho ‚àöd_k (scaling factor)
      + √Åp d·ª•ng softmax ƒë·ªÉ c√≥ weights cho values
    -L√Ω do scaling: Khi d_k l·ªõn, dot products c√≥ magnitude l·ªõn, ƒë·∫©y softmax v√†o v√πng gradients c·ª±c nh·ªè
    -So s√°nh v·ªõi Additive attention: Dot-product nhanh h∆°n v√† space-efficient h∆°n (c√≥ th·ªÉ d√πng matrix multiplication), nh∆∞ng c·∫ßn scaling khi d_k l·ªõn

    |-3.2.2. Multi-Head Attention
    -Thay v√¨ single attention function v·ªõi d_model dimensions
    -Th·ª±c hi·ªán h l·∫ßn attention song song v·ªõi c√°c learned linear projections kh√°c nhau:
      + Queries, keys, values ƒë∆∞·ª£c project xu·ªëng d_k, d_k, d_v dimensions (h l·∫ßn)
      + M·ªói projected version ch·∫°y attention song song ‚Üí d_v-dimensional output
      + Concatenate v√† project l·∫°i
    -C√¥ng th·ª©c: 
      + MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O
      + head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
    -Trong paper: h=8 parallel attention layers (heads), d_k = d_v = d_model/h = 64
    -√ù nghƒ©a: Cho ph√©p model attend to information t·ª´ c√°c representation subspaces kh√°c nhau t·∫°i c√°c positions kh√°c nhau
    -Chi ph√≠ t√≠nh to√°n t∆∞∆°ng t·ª± single-head attention v·ªõi full dimensionality do dimension b·ªã gi·∫£m

    |-3.2.3. Application of Attention in our Model
    -Transformer s·ª≠ d·ª•ng multi-head attention theo 3 c√°ch kh√°c nhau:
    1- Encoder-decoder attention layers:
       + Queries t·ª´ decoder layer tr∆∞·ªõc
       + Keys v√† values t·ª´ output c·ªßa encoder
       + Cho ph√©p m·ªçi position trong decoder attend to t·∫•t c·∫£ positions trong input sequence
       + M√¥ ph·ªèng encoder-decoder attention mechanisms ƒëi·ªÉn h√¨nh trong seq2seq models
    2- Encoder self-attention layers:
       + Keys, values, queries ƒë·ªÅu t·ª´ output c·ªßa encoder layer tr∆∞·ªõc
       + M·ªói position trong encoder c√≥ th·ªÉ attend to t·∫•t c·∫£ positions trong encoder layer tr∆∞·ªõc
    3- Decoder self-attention layers:
       + T∆∞∆°ng t·ª± encoder nh∆∞ng c√≥ masking
       + M·ªói position trong decoder ch·ªâ attend to t·∫•t c·∫£ positions ‚â§ position ƒë√≥
       + NgƒÉn information flow t·ª´ t∆∞∆°ng lai, preserve auto-regressive property
    
|-3.3. Position-wise Feed-Forward Networks
Multilayer-Persceptron l√† g√¨?

Feed-Forward Networks l√† g√¨?

Postion-wise Feed-Forward Networks l√† g√¨?

-M·ªói layer trong encoder v√† decoder ch·ª©a fully connected feed-forward network
-√Åp d·ª•ng ri√™ng r·∫Ω v√† gi·ªëng h·ªát nhau cho t·ª´ng position
-G·ªìm 2 linear transformations v·ªõi ReLU activation ·ªü gi·ªØa:
  + FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
-Linear transformations gi·ªëng nhau across positions kh√°c nhau, nh∆∞ng s·ª≠ d·ª•ng parameters kh√°c nhau gi·ªØa c√°c layers
-C√≥ th·ªÉ coi nh∆∞ hai convolutions v·ªõi kernel size 1
-Input v√† output dimensionality d_model = 512
-Inner-layer dimensionality d_ff = 2048

|-3.4. Embeddings and Softmax  
-S·ª≠ d·ª•ng learned embeddings ƒë·ªÉ convert input tokens v√† output tokens th√†nh vectors c√≥ dimension d_model = 512
-S·ª≠ d·ª•ng learned linear transformation v√† softmax ƒë·ªÉ convert decoder output th√†nh predicted next-token probabilities
-Chia s·∫ª c√πng weight matrix gi·ªØa 2 embedding layers v√† pre-softmax linear transformation (t∆∞∆°ng t·ª± nh∆∞ trong c√°c papers kh√°c)
-Trong embedding layers, nh√¢n weights ƒë√≥ v·ªõi ‚àöd_model

|-3.5. Positional Encoding
Positional Encoding l√† g√¨?
-Do model kh√¥ng c√≥ recurrence v√† convolution, c·∫ßn inject information v·ªÅ relative/absolute position c·ªßa tokens trong sequence
-"Positional encodings" ƒë∆∞·ª£c th√™m v√†o input embeddings ·ªü ƒë√°y encoder v√† decoder stacks
-Positional encodings c√≥ c√πng dimension d_model = 512 v·ªõi embeddings ‚Üí c√≥ th·ªÉ summed
-Nhi·ªÅu l·ª±a ch·ªçn: learned v√† fixed positional encodings
-Paper s·ª≠ d·ª•ng sine v√† cosine functions v·ªõi frequencies kh√°c nhau:
  + PE(pos, 2i) = sin(pos/10000^(2i/d_model))
  + PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))
  + pos l√† position, i l√† dimension
-L√Ω do ch·ªçn sinusoidal version:
  + C√≥ th·ªÉ cho ph√©p model extrapolate ƒë·∫øn sequence lengths d√†i h∆°n nh·ªØng g√¨ g·∫∑p trong training
  + V·ªõi m·ªçi fixed offset k, PE(pos+k) c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn nh∆∞ linear function c·ªßa PE(pos)
-Th·ª≠ nghi·ªám v·ªõi learned positional embeddings cho k·∫øt qu·∫£ g·∫ßn nh∆∞ gi·ªëng h·ªát

4. Why Self-Attention 
-So s√°nh self-attention layers v·ªõi recurrent v√† convolutional layers theo 3 ti√™u ch√≠:
1- Complexity per layer (ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n m·ªói layer)
2- Amount of computation c√≥ th·ªÉ parallelized (l∆∞·ª£ng t√≠nh to√°n song song h√≥a ƒë∆∞·ª£c) - measured by minimum number of sequential operations required
3- Path length gi·ªØa long-range dependencies trong network

-Ph√¢n t√≠ch chi ti·∫øt:
  + Self-Attention: 
    * Complexity: O(n¬≤¬∑d) - n l√† sequence length, d l√† representation dimension
    * Sequential Operations: O(1)
    * Maximum Path Length: O(1)
  + Recurrent: 
    * Complexity: O(n¬∑d¬≤)
    * Sequential Operations: O(n)
    * Maximum Path Length: O(n)
  + Convolutional: 
    * Complexity: O(k¬∑n¬∑d¬≤) - k l√† kernel width
    * Sequential Operations: O(1)
    * Maximum Path Length: O(log_k(n)) v·ªõi dilated convolutions
  + Self-Attention (restricted):
    * Maximum Path Length: O(n/r) khi ch·ªâ xem x√©t neighborhood size r

-K·∫øt lu·∫≠n:
  + Khi n < d (th∆∞·ªùng x·∫£y ra v·ªõi sentence representations - d=512, 1024), self-attention nhanh h∆°n recurrent layers
  + Self-attention c√≥ constant sequential operations v√† path length ‚Üí h·ªçc long-range dependencies hi·ªáu qu·∫£ h∆°n
  + Side-benefit: Self-attention c√≥ th·ªÉ yield more interpretable models (c√≥ th·ªÉ visualize attention distributions)

5. Training

|-5.1. Training data and Batching
-Training Data:
  + WMT 2014 English-German dataset: ~4.5 tri·ªáu sentence pairs
    * Sentences ƒë∆∞·ª£c encode b·∫±ng byte-pair encoding
    * Shared source-target vocabulary ~37000 tokens
  + WMT 2014 English-French dataset: ~36 tri·ªáu sentences
    * Tokens ƒë∆∞·ª£c split th√†nh 32000 word-piece vocabulary
-Batching:
  + Sentence pairs ƒë∆∞·ª£c batch theo approximate sequence length
  + M·ªói training batch ch·ª©a ~25000 source tokens v√† 25000 target tokens

|-5.2. Hardware Schedule
-Hardware: 8 NVIDIA P100 GPUs
-Training time:
  + Base models: ~12 gi·ªù (m·ªói training step ~0.4 gi√¢y)
  + Big models: 3.5 ng√†y (m·ªói training step ~1.0 gi√¢y)
  + T·ªïng c·ªông 100,000 steps (~300,000 steps cho big model)

|-5.3. Optimizer
-S·ª≠ d·ª•ng Adam optimizer v·ªõi Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.98, Œµ=10‚Åª‚Åπ
-Vary learning rate trong qu√° tr√¨nh training theo c√¥ng th·ª©c:
  lrate = d_model^(-0.5) ¬∑ min(step_num^(-0.5), step_num ¬∑ warmup_steps^(-1.5))
-Learning rate tƒÉng linearly trong warmup_steps ƒë·∫ßu ti√™n, sau ƒë√≥ gi·∫£m t·ª∑ l·ªá ngh·ªãch v·ªõi square root c·ªßa step number
-S·ª≠ d·ª•ng warmup_steps = 4000

|-5.4. Regularization
-√Åp d·ª•ng 3 lo·∫°i regularization:
1- Residual Dropout:
   + √Åp d·ª•ng dropout cho output c·ªßa m·ªói sub-layer, tr∆∞·ªõc khi add v·ªõi sub-layer input v√† normalize
   + √Åp d·ª•ng dropout cho sums c·ªßa embeddings v√† positional encodings trong encoder v√† decoder stacks
   + Base model: P_drop = 0.1
   
2- Label Smoothing:
   + S·ª≠ d·ª•ng label smoothing value Œµ_ls = 0.1 trong training
   + L√†m hurt perplexity (model tr·ªü n√™n uncertain h∆°n)
   + Nh∆∞ng c·∫£i thi·ªán accuracy v√† BLEU score
   + Gi√∫p model h·ªçc generalize t·ªët h∆°n

6. Results
|-6.1. Machine Translation
-WMT 2014 English-to-German translation:
  + Big transformer model: BLEU = 28.4
  + C·∫£i thi·ªán h∆°n 2.0 BLEU so v·ªõi best previously reported models (bao g·ªìm ensembles)
  + Training cost: 3.5 ng√†y tr√™n 8 P100 GPUs (fraction c·ªßa training costs c·ªßa best models t·ª´ literature)
  
-WMT 2014 English-to-French translation:
  + Big model: BLEU = 41.0
  + Outperforms t·∫•t c·∫£ previously published single models
  + Training cost < 1/4 training cost c·ªßa previous state-of-the-art model
  
-Base model:
  + Training: 10 minutes/epoch (~12 gi·ªù total)
  + K·∫øt qu·∫£ v·∫´n surpasses nhi·ªÅu published models v√† ensembles
  
-Ensemble averaging:
  + S·ª≠ d·ª•ng 8 checkpoints cu·ªëi c√πng (saved 10 minutes intervals)
  + English-to-German: BLEU = 28.4 ‚Üí 30.2 (v·ªõi beam search v√† length penalty Œ±=0.6)
  + English-to-French: BLEU = 41.0 ‚Üí 41.8

|-6.2. Model Variations
-Th·ª±c hi·ªán ablation study ƒë·ªÉ ƒë√°nh gi√° t·∫ßm quan tr·ªçng c·ªßa c√°c components kh√°c nhau
-Experiments v·ªõi variations c·ªßa Transformer base model:

1- Varying number of attention heads:
   + Single-head attention: gi·∫£m 0.9 BLEU (worse quality)
   + Too many heads (h=32): c≈©ng gi·∫£m quality
   + h=8 l√† optimal

2- Reducing attention key size (d_k):
   + L√†m hurt model quality
   + Determining compatibility kh√≥ h∆°n khi dimension nh·ªè h∆°n

3- Model size (d_model, d_ff):
   + Bigger models t·ªët h∆°n
   + Dropout r·∫•t h·ªØu √≠ch trong vi·ªác avoid overfitting

4- Replacing sinusoidal positional encoding v·ªõi learned positional embeddings:
   + K·∫øt qu·∫£ g·∫ßn nh∆∞ gi·ªëng h·ªát base model
   
-K·∫øt lu·∫≠n: Multi-head attention, appropriate key dimensions, v√† model size ƒë·ªÅu quan tr·ªçng

|-6.3. English Constituency Parsing 
-ƒê·ªÉ ƒë√°nh gi√° kh·∫£ nƒÉng generalization c·ªßa Transformer sang tasks kh√°c ngo√†i machine translation
-Th·ª±c hi·ªán experiments tr√™n English constituency parsing
-Task n√†y ƒë·∫∑c th√π:
  + Output b·ªã constraints m·∫°nh
  + D√†i h∆°n ƒë√°ng k·ªÉ so v·ªõi input
  + RNN seq2seq models ch∆∞a c√≥ th·ªÉ achieve state-of-the-art results
  
-Setup:
  + Train tr√™n Wall Street Journal (WSJ) portion c·ªßa Penn Treebank
  + ~40K training sentences
  + Semi-supervised setting: s·ª≠ d·ª•ng th√™m BerkleyParser corpus v·ªõi ~17M sentences
  
-K·∫øt qu·∫£:
  + M·∫∑c d√π thi·∫øu task-specific tuning
  + Transformer (4-layer, d_model=1024) ƒë·∫°t k·∫øt qu·∫£ excellent
  + Outperforms t·∫•t c·∫£ previously reported models NGO·∫†I TR·ª™ Recurrent Neural Network Grammar
  + WSJ only setting: F1 = 91.3
  + Semi-supervised setting: F1 = 92.7 (Recurrent Neural Network Grammar ƒë·∫°t 93.3)
  
-√ù nghƒ©a: Ch·ª©ng minh Transformer generalizes well sang c√°c tasks kh√°c

7. Conclusion
-Transformer: first sequence transduction model d·ª±a ho√†n to√†n tr√™n attention, thay th·∫ø recurrent layers b·∫±ng multi-headed self-attention
-∆Øu ƒëi·ªÉm:
  + Train nhanh h∆°n ƒë√°ng k·ªÉ so v·ªõi architectures based on recurrent/convolutional layers
  + ƒê·∫°t state-of-the-art quality tr√™n translation tasks
  + English-to-German: BLEU 28.4 (improve 2.0 BLEU)
  + English-to-French: BLEU 41.8 (new state-of-the-art)
  
-Excited v·ªÅ t∆∞∆°ng lai c·ªßa attention-based models:
  + Plan √°p d·ª•ng cho problems kh√°c ngo√†i text
  + Plan investigate local, restricted attention mechanisms ƒë·ªÉ efficiently handle large inputs/outputs nh∆∞ images, audio, video
  + Making generation less sequential l√† m·ªôt research goal kh√°c
  
-Code availability:
  + Code ƒë·ªÉ train v√† evaluate models c√≥ t·∫°i: https://github.com/tensorflow/tensor2tensor

-T·∫ßm quan tr·ªçng l·ªãch s·ª≠:
  + Paper n√†y ƒë√£ m·ªü ra k·ª∑ nguy√™n m·ªõi cho NLP v√† AI
  + Ki·∫øn tr√∫c Transformer tr·ªü th√†nh n·ªÅn t·∫£ng cho BERT, GPT, T5 v√† v√¥ s·ªë models kh√°c
  + Ch·ª©ng minh r·∫±ng attention mechanism l√† ƒë·ªß m·∫°nh, kh√¥ng nh·∫•t thi·∫øt c·∫ßn recurrence
  + ƒê·∫∑t n·ªÅn m√≥ng cho Large Language Models (LLMs) hi·ªán ƒë·∫°i

References

=====================================================================================
                    T√ìM T·∫ÆT V√Ä PH√ÇN T√çCH B√ÄI B√ÅO "ATTENTION IS ALL YOU NEED"
=====================================================================================

**PH·∫¶N 1: T·ªîNG QUAN (ABSTRACT)**

B√†i b√°o gi·ªõi thi·ªáu m√¥ h√¨nh m·ªõi mang t√™n: TRANSFORMER

K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c (ƒë√°nh gi√° b·∫±ng ƒëi·ªÉm BLEU - ch·ªâ s·ªë ƒëo l∆∞·ªùng ch·∫•t l∆∞·ª£ng d·ªãch m√°y):
‚Ä¢ D·ªãch Anh ‚Üí ƒê·ª©c: 28.4 ƒëi·ªÉm (WMT 2014)
‚Ä¢ D·ªãch Anh ‚Üí Ph√°p: 41.8 ƒëi·ªÉm (hu·∫•n luy·ªán 3.5 ng√†y tr√™n 8 GPUs)
‚Ä¢ K·∫øt qu·∫£ v∆∞·ª£t tr·ªôi c·∫£ tr√™n b·ªô d·ªØ li·ªáu l·ªõn v√† nh·ªè

=====================================================================================

**PH·∫¶N 2: GI·ªöI THI·ªÜU (INTRODUCTION)**

2.1. V·∫§N ƒê·ªÄ C·ª¶A C√ÅC M√î H√åNH C≈®

C√°c m√¥ h√¨nh tr∆∞·ªõc ƒë√¢y (RNN, LSTM, GRU) c√≥ nh∆∞·ª£c ƒëi·ªÉm l·ªõn:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ T√çNH TO√ÅN TU·∫¶N T·ª∞ (Sequential Processing)                   ‚îÇ
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ Token 1 ‚Üí Token 2 ‚Üí Token 3 ‚Üí ... ‚Üí Token n                ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ ‚ùå Ph·∫£i x·ª≠ l√Ω l·∫ßn l∆∞·ª£t t·ª´ng token                            ‚îÇ
‚îÇ ‚ùå Kh√¥ng th·ªÉ song song h√≥a (parallelize)                    ‚îÇ
‚îÇ ‚ùå V·ªõi c√¢u d√†i, th√¥ng tin ƒë·∫ßu c√¢u b·ªã "qu√™n" m·∫•t            ‚îÇ
‚îÇ    (vanishing gradient problem)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

V√ç D·ª§: Khi x·ª≠ l√Ω c√¢u "Con m√®o m√†u ƒëen ƒëang ng·ªß tr√™n gh·∫ø sofa"
- Khi model x·ª≠ l√Ω ƒë·∫øn t·ª´ "gh·∫ø", n√≥ c√≥ th·ªÉ ƒë√£ qu√™n th√¥ng tin v·ªÅ "con m√®o"
- Ph·∫£i ch·ªù x·ª≠ l√Ω xong "con" m·ªõi x·ª≠ l√Ω "m√®o", r·ªìi m·ªõi ƒë·∫øn "m√†u"...

2.2. GI·∫¢I PH√ÅP C·ª¶A TRANSFORMER

C∆° ch·∫ø Attention ƒë√£ ƒë∆∞·ª£c d√πng tr∆∞·ªõc ƒë√≥ ƒë·ªÉ c·∫£i thi·ªán RNN.
‚Üí Transformer ƒëi xa h∆°n: B·ªé HO√ÄN TO√ÄN RNN/CNN, CH·ªà D√ôNG ATTENTION!

=====================================================================================

**PH·∫¶N 3: B·ªêI C·∫¢NH L·ªäCH S·ª¨ (BACKGROUND)**

3.1. C√°c m√¥ h√¨nh truy·ªÅn th·ªëng: RNN/CNN v·ªõi encoder-decoder
3.2. C√°c m√¥ h√¨nh t·ªët nh·∫•t ƒë·ªÅu d√πng Attention ƒë·ªÉ k·∫øt n·ªëi encoder-decoder
3.3. M·ªôt s·ªë m√¥ h√¨nh d√πng CNN (Extended Neural GPU, ByteNet, ConvS2S):
     ‚úì T√≠nh to√°n song song ƒë∆∞·ª£c
     ‚úó S·ªë b∆∞·ªõc ƒë·ªÉ h·ªçc m·ªëi quan h·ªá xa tƒÉng theo kho·∫£ng c√°ch:
       ‚Ä¢ ConvS2S: t·ª∑ l·ªá tuy·∫øn t√≠nh (linear) v·ªõi kho·∫£ng c√°ch
       ‚Ä¢ ByteNet: t·ª∑ l·ªá logarit (logarithmic)
       
3.4. **Transformer kh√°c bi·ªát nh∆∞ th·∫ø n√†o?**
     ‚úì S·ªë b∆∞·ªõc h·ªçc m·ªëi quan h·ªá: CONSTANT (kh√¥ng ph·ª• thu·ªôc kho·∫£ng c√°ch!)
     ‚úì D√πng Multi-Head Attention ƒë·ªÉ b√π ƒë·∫Øp vi·ªác averaging
     
3.5. **Self-Attention l√† g√¨?**
     ‚Üí C∆° ch·∫ø attention gi√∫p c√°c v·ªã tr√≠ trong C√ôNG m·ªôt c√¢u "nh√¨n th·∫•y nhau"
     ‚Üí T√≠nh to√°n m·ªëi li√™n h·ªá gi·ªØa c√°c t·ª´ v·ªõi nhau trong c√πng 1 sequence

3.6. **ƒê·ªôt ph√° c·ªßa Transformer:**
     M√¥ h√¨nh ƒë·∫ßu ti√™n CH·ªà D√ôNG self-attention, KH√îNG D√ôNG RNN/CNN!

=====================================================================================

**PH·∫¶N 4: KI·∫æN TR√öC M√î H√åNH (MODEL ARCHITECTURE)**

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    KI·∫æN TR√öC T·ªîNG QUAN TRANSFORMER                    ‚îÇ
‚îÇ                                                                       ‚îÇ
‚îÇ  INPUT ‚Üí ENCODER (6 layers) ‚Üí DECODER (6 layers) ‚Üí OUTPUT           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

4.1. ENCODER V√Ä DECODER STACKS

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
A. ENCODER (B·ªô m√£ h√≥a)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

C·∫•u tr√∫c: Ch·ªìng 6 layers gi·ªëng h·ªát nhau

M·ªói layer c√≥ 2 sub-layers:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Sub-layer 1: Multi-head Self-Attention        ‚îÇ
‚îÇ  (Gi√∫p c√°c t·ª´ "nh√¨n th·∫•y" nhau trong c√¢u)     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Sub-layer 2: Feed-Forward Network             ‚îÇ
‚îÇ  (X·ª≠ l√Ω t·ª´ng v·ªã tr√≠ ƒë·ªôc l·∫≠p)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üîß K·ª∏ THU·∫¨T QUAN TR·ªåNG:
‚Ä¢ Residual Connection: Output = LayerNorm(x + Sublayer(x))
  ‚Üí Gi√∫p training d·ªÖ h∆°n, tr√°nh vanishing gradient
‚Ä¢ Layer Normalization: Chu·∫©n h√≥a output
‚Ä¢ Dimension c·ªßa output: d_model = 512 (·ªü t·∫•t c·∫£ c√°c layers)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
B. DECODER (B·ªô gi·∫£i m√£)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

C·∫•u tr√∫c: Ch·ªìng 6 layers gi·ªëng h·ªát nhau

M·ªói layer c√≥ 3 sub-layers (nhi·ªÅu h∆°n Encoder 1 sub-layer):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Sub-layer 1: MASKED Multi-head Self-Attention         ‚îÇ
‚îÇ  (Ch·ªâ nh√¨n c√°c t·ª´ ·ªü TR∆Ø·ªöC, kh√¥ng nh√¨n t∆∞∆°ng lai)      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Sub-layer 2: Multi-head Attention (Encoder-Decoder)   ‚îÇ
‚îÇ  (Nh·∫≠n th√¥ng tin t·ª´ Encoder)                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Sub-layer 3: Feed-Forward Network                     ‚îÇ
‚îÇ  (X·ª≠ l√Ω t·ª´ng v·ªã tr√≠ ƒë·ªôc l·∫≠p)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚ö†Ô∏è T·∫†I SAO C·∫¶N MASKING?
V√ç D·ª§: Khi d·ªãch "I love you" ‚Üí "T√¥i y√™u b·∫°n"
‚Ä¢ Khi t·∫°o t·ª´ "y√™u", model CH·ªà ƒë∆∞·ª£c nh√¨n "T√¥i" (t·ª´ ƒë√£ t·∫°o)
‚Ä¢ KH√îNG ƒë∆∞·ª£c nh√¨n "b·∫°n" (t·ª´ ch∆∞a t·∫°o - thu·ªôc t∆∞∆°ng lai)
‚Ä¢ ƒê√¢y l√† t√≠nh ch·∫•t auto-regressive (t·ª± h·ªìi quy)

=====================================================================================

4.2. C∆† CH·∫æ ATTENTION (PH·∫¶N QUAN TR·ªåNG NH·∫§T!)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
4.2.1. SCALED DOT-PRODUCT ATTENTION
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üìö TR·∫¢ L·ªúI C√ÇU H·ªéI C·ª¶A B·∫†N:

‚ùì EMBEDDING L√Ä G√å?
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Embedding = Chuy·ªÉn ƒë·ªïi t·ª´ vƒÉn b·∫£n sang vector s·ªë

V√ç D·ª§ ƒë∆°n gi·∫£n:
‚Ä¢ T·ª´ "m√®o" ‚Üí [0.2, 0.8, 0.1, 0.5, ...] (512 s·ªë)
‚Ä¢ T·ª´ "ch√≥" ‚Üí [0.3, 0.7, 0.2, 0.4, ...] (512 s·ªë)

T·∫°i sao c·∫ßn Embedding?
‚Üí M√°y t√≠nh kh√¥ng hi·ªÉu ch·ªØ, ch·ªâ hi·ªÉu s·ªë!
‚Üí Embedding bi·∫øn t·ª´ th√†nh vector ƒë·ªÉ m√°y t√≠nh x·ª≠ l√Ω ƒë∆∞·ª£c
‚Üí C√°c t·ª´ c√≥ nghƒ©a g·∫ßn nhau s·∫Ω c√≥ vector g·∫ßn nhau

V√ç D·ª§ th·ª±c t·∫ø:
"vua" v√† "ho√†ng ƒë·∫ø" ‚Üí vectors g·∫ßn nhau
"vua" v√† "xe ƒë·∫°p" ‚Üí vectors xa nhau


‚ùì DOT-PRODUCT ·ªû ƒê√ÇY L√Ä G√å?
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Dot-Product (T√≠ch v√¥ h∆∞·ªõng) = Ph√©p nh√¢n gi·ªØa 2 vectors

C√¥ng th·ª©c c∆° b·∫£n:
A = [a‚ÇÅ, a‚ÇÇ, a‚ÇÉ]
B = [b‚ÇÅ, b‚ÇÇ, b‚ÇÉ]
A ¬∑ B = a‚ÇÅ√ób‚ÇÅ + a‚ÇÇ√ób‚ÇÇ + a‚ÇÉ√ób‚ÇÉ

Trong Attention:
‚Üí T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa Query v√† Key
‚Üí K·∫øt qu·∫£ c√†ng l·ªõn = 2 vector c√†ng gi·ªëng nhau = c√†ng li√™n quan

V√ç D·ª§:
Vector "m√®o" ¬∑ Vector "ƒë·ªông v·∫≠t" = s·ªë l·ªõn (li√™n quan m·∫°nh)
Vector "m√®o" ¬∑ Vector "√¥ t√¥" = s·ªë nh·ªè (√≠t li√™n quan)


‚ùì QUERY, KEY, VALUE L√Ä G√å?
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
H√£y t∆∞·ªüng t∆∞·ª£ng nh∆∞ T√åM KI·∫æM TR√äN YOUTUBE:

‚Ä¢ QUERY (Truy v·∫•n):
  ‚Üí ƒêi·ªÅu b·∫°n ƒëang T√åM KI·∫æM
  ‚Üí VD: "C√°ch n·∫•u ph·ªü b√≤"

‚Ä¢ KEY (Ch√¨a kh√≥a):
  ‚Üí TI√äU ƒê·ªÄ c·ªßa m·ªói video
  ‚Üí VD: "H∆∞·ªõng d·∫´n n·∫•u ph·ªü b√≤ chu·∫©n v·ªã", "C√°ch l√†m b√°nh m√¨"...

‚Ä¢ VALUE (Gi√° tr·ªã):
  ‚Üí N·ªòI DUNG th·ª±c s·ª± c·ªßa video
  ‚Üí Th√¥ng tin b·∫°n mu·ªën l·∫•y

C√ÅCH HO·∫†T ƒê·ªòNG:
1. So s√°nh QUERY v·ªõi t·∫•t c·∫£ KEYs (b·∫±ng dot-product)
2. Key n√†o gi·ªëng Query nh·∫•t ‚Üí c√≥ ƒëi·ªÉm cao nh·∫•t
3. L·∫•y VALUE c·ªßa nh·ªØng Key c√≥ ƒëi·ªÉm cao
4. K·∫øt h·ª£p c√°c VALUE theo t·ª∑ l·ªá ƒëi·ªÉm s·ªë

Trong Transformer:
‚Ä¢ Query = "T·ª´ hi·ªán t·∫°i ƒëang xem x√©t"
‚Ä¢ Keys = "T·∫•t c·∫£ c√°c t·ª´ kh√°c trong c√¢u"
‚Ä¢ Values = "Th√¥ng tin c·ªßa c√°c t·ª´ ƒë√≥"


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
C√îNG TH·ª®C SCALED DOT-PRODUCT ATTENTION:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

                    Attention(Q, K, V) = softmax(QK·µÄ / ‚àöd‚Çñ) V

GI·∫¢I TH√çCH T·ª™NG B∆Ø·ªöC:

B∆∞·ªõc 1: QK·µÄ
‚Üí T√≠nh dot-product gi·ªØa Query v√† t·∫•t c·∫£ Keys
‚Üí K·∫øt qu·∫£: Ma tr·∫≠n ƒëi·ªÉm s·ªë (scores matrix)

B∆∞·ªõc 2: QK·µÄ / ‚àöd‚Çñ
‚Üí CHIA cho ‚àöd‚Çñ (d‚Çñ = 64, n√™n ‚àöd‚Çñ = 8)
‚Üí T·∫°i sao? ƒê·ªÉ ƒëi·ªÉm s·ªë kh√¥ng qu√° l·ªõn (tr√°nh softmax b·ªã "b√£o h√≤a")

B∆∞·ªõc 3: softmax(...)
‚Üí Chuy·ªÉn ƒëi·ªÉm s·ªë th√†nh x√°c su·∫•t (t·ªïng = 1)
‚Üí Attention weights (tr·ªçng s·ªë attention)

B∆∞·ªõc 4: ... √ó V
‚Üí L·∫•y trung b√¨nh c√≥ tr·ªçng s·ªë c·ªßa c√°c Values
‚Üí K·∫øt qu·∫£ cu·ªëi c√πng!


V√ç D·ª§ C·ª§ TH·ªÇ:
C√¢u: "Con m√®o ƒëang ng·ªß"
T√≠nh attention cho t·ª´ "m√®o":

Query = vector c·ªßa "m√®o"
Keys = vectors c·ªßa ["Con", "m√®o", "ƒëang", "ng·ªß"]
Values = vectors c·ªßa ["Con", "m√®o", "ƒëang", "ng·ªß"]

ƒêi·ªÉm s·ªë sau dot-product (gi·∫£ ƒë·ªãnh):
"m√®o" v·ªõi "Con" = 0.1
"m√®o" v·ªõi "m√®o" = 0.9  (cao nh·∫•t - t·ª± nh√¨n m√¨nh)
"m√®o" v·ªõi "ƒëang" = 0.3
"m√®o" v·ªõi "ng·ªß" = 0.6  (kh√° cao - "m√®o" li√™n quan ƒë·∫øn "ng·ªß")

Sau softmax ‚Üí Attention weights:
[0.05, 0.45, 0.15, 0.35]

Output = 0.05√óV_Con + 0.45√óV_m√®o + 0.15√óV_ƒëang + 0.35√óV_ng·ªß


üìä SO S√ÅNH V·ªöI ADDITIVE ATTENTION:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      ‚îÇ Dot-Product    ‚îÇ Additive        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ T·ªëc ƒë·ªô               ‚îÇ ‚ö° R·∫•t nhanh   ‚îÇ üêå Ch·∫≠m h∆°n     ‚îÇ
‚îÇ Hi·ªáu qu·∫£ b·ªô nh·ªõ      ‚îÇ ‚úì T·ªët         ‚îÇ ‚úó K√©m h∆°n       ‚îÇ
‚îÇ Khi d‚Çñ l·ªõn           ‚îÇ C·∫ßn scaling    ‚îÇ ·ªîn ƒë·ªãnh h∆°n     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚Üí Paper ch·ªçn Dot-Product v√¨ nhanh h∆°n (d√πng matrix multiplication)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
4.2.2. MULTI-HEAD ATTENTION (ƒêA ƒê·∫¶U ATTENTION)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üí° √ù T∆Ø·ªûNG CH√çNH:
Thay v√¨ 1 l·∫ßn attention v·ªõi 512 dimensions
‚Üí L√†m 8 l·∫ßn attention song song, m·ªói l·∫ßn v·ªõi 64 dimensions!

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         1 Attention (512 dims)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         VS
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Head 1  ‚îÇ Head 2  ‚îÇ Head 3  ‚îÇ Head 4  ‚îÇ  ...    ‚îÇHead 8‚îÇ
‚îÇ (64dim) ‚îÇ (64dim) ‚îÇ (64dim) ‚îÇ (64dim) ‚îÇ         ‚îÇ(64d) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

T·∫†I SAO L√ÄM NH∆Ø V·∫¨Y?
‚Üí M·ªói head h·ªçc m·ªôt ki·ªÉu m·ªëi quan h·ªá kh√°c nhau!

V√ç D·ª§ trong c√¢u "The animal didn't cross the street because it was too tired":

Head 1: "it" ‚Üí "animal" (ch·ªß ng·ªØ)
Head 2: "it" ‚Üí "tired" (tr·∫°ng th√°i)
Head 3: "cross" ‚Üí "street" (ƒë·ªông t·ª´-ƒë·ªëi t∆∞·ª£ng)
Head 4: "too" ‚Üí "tired" (m·ª©c ƒë·ªô)
... v√† c√≤n nhi·ªÅu m·ªëi quan h·ªá kh√°c!


C√îNG TH·ª®C:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MultiHead(Q, K, V) = Concat(head‚ÇÅ, head‚ÇÇ, ..., head‚Çà) W·¥º

Trong ƒë√≥:
    head·µ¢ = Attention(Q¬∑W·µ¢·µ†, K¬∑W·µ¢·¥∑, V¬∑W·µ¢‚±Ω)

Q¬∑W·µ¢·µ†: Project Query xu·ªëng 64 dimensions cho head th·ª© i
K¬∑W·µ¢·¥∑: Project Key xu·ªëng 64 dimensions cho head th·ª© i
V¬∑W·µ¢‚±Ω: Project Value xu·ªëng 64 dimensions cho head th·ª© i


TH√îNG S·ªê TRONG PAPER:
‚Ä¢ S·ªë heads: h = 8
‚Ä¢ Dimension m·ªói head: d‚Çñ = d·µ• = 64
‚Ä¢ T·ªïng dimension: 8 √ó 64 = 512 = d_model

‚ö° CHI PH√ç T√çNH TO√ÅN:
M·∫∑c d√π c√≥ 8 heads, nh∆∞ng dimension m·ªói head gi·∫£m xu·ªëng 1/8
‚Üí Chi ph√≠ t√≠nh to√°n t∆∞∆°ng ƒë∆∞∆°ng 1 head v·ªõi full dimension!

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
4.2.3. ·ª®NG D·ª§NG C·ª¶A ATTENTION TRONG TRANSFORMER
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Transformer s·ª≠ d·ª•ng Multi-Head Attention ·ªü 3 v·ªã tr√≠:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1Ô∏è‚É£ ENCODER-DECODER ATTENTION                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Queries: t·ª´ Decoder layer tr∆∞·ªõc                           ‚îÇ
‚îÇ ‚Ä¢ Keys & Values: t·ª´ output c·ªßa Encoder                      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ M·ª§C ƒê√çCH: Decoder "nh√¨n v√†o" to√†n b·ªô input sequence        ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ VD: Khi d·ªãch "I love you" ‚Üí "T√¥i y√™u b·∫°n"                 ‚îÇ
‚îÇ     Khi t·∫°o t·ª´ "y√™u", decoder nh√¨n v√†o ["I","love","you"] ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2Ô∏è‚É£ ENCODER SELF-ATTENTION                                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Q, K, V: ƒë·ªÅu t·ª´ output c·ªßa encoder layer tr∆∞·ªõc           ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ M·ª§C ƒê√çCH: M·ªói t·ª´ "nh√¨n th·∫•y" t·∫•t c·∫£ c√°c t·ª´ kh√°c trong input‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ VD: Trong c√¢u "The bank by the river"                      ‚îÇ
‚îÇ     T·ª´ "bank" nh√¨n th·∫•y "river" ‚Üí hi·ªÉu l√† "b·ªù s√¥ng"       ‚îÇ
‚îÇ     (kh√¥ng ph·∫£i "ng√¢n h√†ng")                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3Ô∏è‚É£ DECODER SELF-ATTENTION (MASKED)                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Q, K, V: t·ª´ decoder layer tr∆∞·ªõc                          ‚îÇ
‚îÇ ‚Ä¢ C√ì MASKING: ch·ªâ nh√¨n c√°c v·ªã tr√≠ ‚â§ v·ªã tr√≠ hi·ªán t·∫°i       ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ M·ª§C ƒê√çCH: Gi·ªØ t√≠nh auto-regressive                         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ VD: Khi t·∫°o t·ª´ "y√™u" (v·ªã tr√≠ 2)                           ‚îÇ
‚îÇ     Ch·ªâ ƒë∆∞·ª£c nh√¨n: "T√¥i" (v·ªã tr√≠ 1) v√† "y√™u" (v·ªã tr√≠ 2)  ‚îÇ
‚îÇ     KH√îNG ƒë∆∞·ª£c nh√¨n: "b·∫°n" (v·ªã tr√≠ 3 - t∆∞∆°ng lai)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

=====================================================================================

4.3. POSITION-WISE FEED-FORWARD NETWORKS

üìö TR·∫¢ L·ªúI C√ÇU H·ªéI C·ª¶A B·∫†N:

‚ùì MULTI-LAYER PERCEPTRON (MLP) L√Ä G√å?
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
MLP = M·∫°ng neural ƒë∆°n gi·∫£n g·ªìm nhi·ªÅu layers k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß

C·∫•u tr√∫c c∆° b·∫£n:
Input Layer ‚Üí Hidden Layer(s) ‚Üí Output Layer

V√ç D·ª§ MLP ƒë∆°n gi·∫£n:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  x‚ÇÅ  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  h‚ÇÅ  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  y‚ÇÅ  ‚îÇ
‚îÇ  x‚ÇÇ  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  h‚ÇÇ  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  y‚ÇÇ  ‚îÇ
‚îÇ  x‚ÇÉ  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  h‚ÇÉ  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Input(3)    Hidden(3)    Output(2)

M·ªói m≈©i t√™n = 1 weight (tr·ªçng s·ªë)
M·ªói node = 1 activation function


‚ùì FEED-FORWARD NETWORK L√Ä G√å?
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Feed-Forward = M·∫°ng neural m√† th√¥ng tin ch·ªâ ƒëi m·ªôt chi·ªÅu

ƒê·∫∑c ƒëi·ªÉm:
‚Ä¢ Th√¥ng tin: Input ‚Üí Hidden ‚Üí Output (m·ªôt chi·ªÅu)
‚Ä¢ KH√îNG c√≥ v√≤ng l·∫∑p (kh√¥ng c√≥ feedback)
‚Ä¢ ƒê∆°n gi·∫£n nh·∫•t trong c√°c lo·∫°i neural networks

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Input ‚Üí Layer 1 ‚Üí Layer 2 ‚Üí Output ‚îÇ  ‚úì Feed-Forward
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Input ‚áÑ Hidden ‚áÑ Output   ‚îÇ  ‚úó Recurrent (c√≥ v√≤ng l·∫∑p)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


‚ùì POSITION-WISE FEED-FORWARD NETWORK L√Ä G√å?
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Position-Wise = √Åp d·ª•ng C√ôNG m·ªôt network cho T·ª™NG v·ªã tr√≠ ƒë·ªôc l·∫≠p

H√£y t∆∞·ªüng t∆∞·ª£ng:
B·∫°n c√≥ c√¢u: ["T√¥i", "y√™u", "m√®o"]

Position-Wise FFN = √Åp d·ª•ng C√ôNG m·ªôt h√†m f() cho t·ª´ng t·ª´:
‚Ä¢ output‚ÇÅ = f("T√¥i")
‚Ä¢ output‚ÇÇ = f("y√™u")
‚Ä¢ output‚ÇÉ = f("m√®o")

Kh√¥ng c√≥ t∆∞∆°ng t√°c gi·ªØa c√°c v·ªã tr√≠!
(S·ª± t∆∞∆°ng t√°c ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi Attention layer)


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
C·∫§U TR√öC CHI TI·∫æT TRONG TRANSFORMER:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

M·ªói layer encoder/decoder c√≥ 1 Position-Wise FFN:

C√îNG TH·ª®C:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
FFN(x) = max(0, x¬∑W‚ÇÅ + b‚ÇÅ)¬∑W‚ÇÇ + b‚ÇÇ

Ho·∫∑c vi·∫øt r√µ h∆°n:
FFN(x) = ReLU(x¬∑W‚ÇÅ + b‚ÇÅ)¬∑W‚ÇÇ + b‚ÇÇ


GI·∫¢I TH√çCH T·ª™NG B∆Ø·ªöC:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ B∆∞·ªõc 1: x¬∑W‚ÇÅ + b‚ÇÅ                              ‚îÇ
‚îÇ ‚Üí Linear transformation ƒë·∫ßu ti√™n              ‚îÇ
‚îÇ ‚Üí Dimension: 512 ‚Üí 2048 (m·ªü r·ªông 4 l·∫ßn!)     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ B∆∞·ªõc 2: ReLU(...)                              ‚îÇ
‚îÇ ‚Üí Activation function                          ‚îÇ
‚îÇ ‚Üí ReLU(x) = max(0, x)                         ‚îÇ
‚îÇ ‚Üí Lo·∫°i b·ªè gi√° tr·ªã √¢m                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ B∆∞·ªõc 3: ...¬∑W‚ÇÇ + b‚ÇÇ                           ‚îÇ
‚îÇ ‚Üí Linear transformation th·ª© hai               ‚îÇ
‚îÇ ‚Üí Dimension: 2048 ‚Üí 512 (thu nh·ªè l·∫°i)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

TH√îNG S·ªê:
‚Ä¢ Input/Output dimension: d_model = 512
‚Ä¢ Hidden dimension: d_ff = 2048
‚Ä¢ K√≠ch th∆∞·ªõc tƒÉng 4 l·∫ßn r·ªìi gi·∫£m v·ªÅ ban ƒë·∫ßu

üí° T·∫†I SAO L√ÄM NH∆Ø V·∫¨Y?
‚Üí TƒÉng expressiveness (kh·∫£ nƒÉng bi·ªÉu di·ªÖn)
‚Üí Cho ph√©p model h·ªçc nh·ªØng pattern ph·ª©c t·∫°p h∆°n

üîß K·ª∏ THU·∫¨T TRI·ªÇN KHAI:
C√≥ th·ªÉ implement b·∫±ng 2 convolutions v·ªõi kernel size = 1
‚Üí Hi·ªáu qu·∫£ h∆°n v·ªÅ m·∫∑t t√≠nh to√°n

=====================================================================================

4.4. EMBEDDINGS V√Ä SOFTMAX

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
EMBEDDING LAYERS:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Chuy·ªÉn ƒë·ªïi tokens ‚Üí vectors 512 chi·ªÅu

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Input Embedding:                             ‚îÇ
‚îÇ "I" ‚Üí [0.1, 0.5, ..., 0.3] (512 numbers)    ‚îÇ
‚îÇ "love" ‚Üí [0.2, 0.8, ..., 0.1] (512 numbers) ‚îÇ
‚îÇ "you" ‚Üí [0.4, 0.3, ..., 0.7] (512 numbers)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Output Embedding:                            ‚îÇ
‚îÇ (T∆∞∆°ng t·ª±, cho target language)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


üîß CHIA S·∫∫ TR·ªåNG S·ªê (Weight Sharing):
3 n∆°i d√πng C√ôNG m·ªôt embedding matrix:
1. Input embedding (Encoder)
2. Output embedding (Decoder)
3. Pre-softmax linear transformation

‚ö†Ô∏è L∆∞u √Ω: Trong embedding layers, nh√¢n weights v·ªõi ‚àöd_model = ‚àö512 ‚âà 22.6


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
OUTPUT & SOFTMAX:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Decoder output ‚Üí Linear transformation ‚Üí Softmax ‚Üí Probabilities

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Decoder output: vector 512 chi·ªÅu          ‚îÇ
‚îÇ            ‚Üì                               ‚îÇ
‚îÇ Linear: 512 ‚Üí vocabulary_size (30000)     ‚îÇ
‚îÇ            ‚Üì                               ‚îÇ
‚îÇ Softmax: chuy·ªÉn th√†nh x√°c su·∫•t           ‚îÇ
‚îÇ            ‚Üì                               ‚îÇ
‚îÇ [P("T√¥i")=0.01, P("Con")=0.02, ...]      ‚îÇ
‚îÇ            ‚Üì                               ‚îÇ
‚îÇ Ch·ªçn t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

=====================================================================================

4.5. POSITIONAL ENCODING (M√É H√ìA V·ªä TR√ç)

üìö TR·∫¢ L·ªúI C√ÇU H·ªéI C·ª¶A B·∫†N:

‚ùì POSITIONAL ENCODING L√Ä G√å?
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

V·∫§N ƒê·ªÄ:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Transformer KH√îNG c√≥ RNN, KH√îNG c√≥ CNN
‚Üí Kh√¥ng bi·∫øt th·ª© t·ª± c·ªßa c√°c t·ª´!

V√ç D·ª§:
"M√®o ƒëu·ªïi ch√≥" v√† "Ch√≥ ƒëu·ªïi m√®o"
‚Üí N·∫øu kh√¥ng bi·∫øt th·ª© t·ª±, model s·∫Ω coi 2 c√¢u n√†y gi·ªëng nhau!

GI·∫¢I PH√ÅP: Positional Encoding
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Th√™m th√¥ng tin v·ªÅ V·ªä TR√ç v√†o m·ªói t·ª´

C√ÅC C√ÅCH TI·∫æP C·∫¨N:

1Ô∏è‚É£ C√°ch ƒë∆°n gi·∫£n (nh∆∞ng kh√¥ng t·ªët):
   Position 1: [1, 0, 0, 0, ...]
   Position 2: [2, 0, 0, 0, ...]
   Position 3: [3, 0, 0, 0, ...]
   ‚Üí V·∫•n ƒë·ªÅ: S·ªë c√†ng l·ªõn, gi√° tr·ªã c√†ng l·ªõn (kh√¥ng c√¥ng b·∫±ng)

2Ô∏è‚É£ C√°ch chu·∫©n h√≥a (v·∫´n kh√¥ng t·ªët):
   Position 1: [0.1, 0, 0, ...]
   Position 2: [0.2, 0, 0, ...]
   ‚Üí V·∫•n ƒë·ªÅ: Ph·ª• thu·ªôc v√†o ƒë·ªô d√†i c√¢u

3Ô∏è‚É£ C√°ch c·ªßa Transformer (T·ªêT NH·∫§T): Sinusoidal
   ‚Üí D√πng h√†m sin v√† cos!


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
C√îNG TH·ª®C POSITIONAL ENCODING:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

Trong ƒë√≥:
‚Ä¢ pos = v·ªã tr√≠ c·ªßa t·ª´ trong c√¢u (0, 1, 2, 3, ...)
‚Ä¢ i = index c·ªßa dimension (0 ƒë·∫øn 255, v√¨ d_model/2 = 256)
‚Ä¢ 2i = dimension ch·∫µn ‚Üí d√πng sin
‚Ä¢ 2i+1 = dimension l·∫ª ‚Üí d√πng cos


V√ç D·ª§ C·ª§ TH·ªÇ:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
T√≠nh PE cho v·ªã tr√≠ pos=1, c√°c dimensions ƒë·∫ßu ti√™n:

Dimension 0 (2i=0, i=0):
PE(1, 0) = sin(1 / 10000^(0/512)) = sin(1) ‚âà 0.841

Dimension 1 (2i+1=1, i=0):
PE(1, 1) = cos(1 / 10000^(0/512)) = cos(1) ‚âà 0.540

Dimension 2 (2i=2, i=1):
PE(1, 2) = sin(1 / 10000^(2/512)) = sin(1/1.045) ‚âà 0.820

...v√† ti·∫øp t·ª•c cho 512 dimensions


H√åNH ·∫¢NH H√ìA:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            Dim 0  Dim 1  Dim 2  Dim 3  ...
Position 0: [0.00, 1.00, 0.00, 1.00, ...]
Position 1: [0.84, 0.54, 0.82, 0.57, ...]
Position 2: [0.91,-0.42, 0.93,-0.36, ...]
Position 3: [0.14,-0.99, 0.20,-0.98, ...]
...

‚Üí M·ªói v·ªã tr√≠ c√≥ m·ªôt "d·∫•u v√¢n tay" ƒë·ªôc nh·∫•t!


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
T·∫†I SAO CH·ªåN SIN/COS?
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚úÖ ∆ØU ƒêI·ªÇM 1: Gi·ªõi h·∫°n trong kho·∫£ng [-1, 1]
   ‚Üí Gi√° tr·ªã kh√¥ng b·ªã b√πng n·ªï

‚úÖ ∆ØU ƒêI·ªÇM 2: Extrapolation (ngo·∫°i suy)
   ‚Üí C√≥ th·ªÉ x·ª≠ l√Ω c√¢u d√†i h∆°n c√¢u trong training
   ‚Üí VD: Training v·ªõi c√¢u t·ªëi ƒëa 100 t·ª´
        Testing v·ªõi c√¢u 150 t·ª´ ‚Üí v·∫´n ho·∫°t ƒë·ªông!

‚úÖ ∆ØU ƒêI·ªÇM 3: M·ªëi quan h·ªá t∆∞∆°ng ƒë·ªëi
   ‚Üí PE(pos + k) c√≥ th·ªÉ bi·ªÉu di·ªÖn nh∆∞ linear function c·ªßa PE(pos)
   ‚Üí Model d·ªÖ h·ªçc kho·∫£ng c√°ch t∆∞∆°ng ƒë·ªëi gi·ªØa c√°c t·ª´

V√ç D·ª§:
   Kho·∫£ng c√°ch t·ª´ v·ªã tr√≠ 5 ƒë·∫øn 10 = 5 b∆∞·ªõc
   Kho·∫£ng c√°ch t·ª´ v·ªã tr√≠ 20 ƒë·∫øn 25 = 5 b∆∞·ªõc
   ‚Üí Model nh·∫≠n ra "pattern" n√†y!


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
C√ÅCH S·ª¨ D·ª§NG TRONG TRANSFORMER:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Embedding + Positional Encoding = Input Vector

V√ç D·ª§ v·ªõi t·ª´ "love" ·ªü v·ªã tr√≠ 1:

Word Embedding("love")  = [0.2, 0.8, 0.1, 0.5, ...]  (512 dims)
Positional Encoding(1)  = [0.84, 0.54, 0.82, 0.57, ...] (512 dims)
                          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Final Input Vector      = [1.04, 1.34, 0.92, 1.07, ...] (C·ªòNG l·∫°i)

‚Üí Th√™m v√†o ·ªü input c·ªßa c·∫£ Encoder v√† Decoder


üìä TH·ª∞C NGHI·ªÜM:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Paper c≈©ng th·ª≠ learned positional embeddings
‚Üí K·∫øt qu·∫£ g·∫ßn nh∆∞ gi·ªëng h·ªát sinusoidal
‚Üí Nh∆∞ng ch·ªçn sinusoidal v√¨ kh·∫£ nƒÉng extrapolate


=====================================================================================

**PH·∫¶N 5: T·∫†I SAO CH·ªåN SELF-ATTENTION?**

Ph·∫ßn n√†y so s√°nh Self-Attention v·ªõi Recurrent v√† Convolutional layers

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
5.1. BA TI√äU CH√ç SO S√ÅNH:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

1Ô∏è‚É£ COMPLEXITY PER LAYER (ƒê·ªô ph·ª©c t·∫°p t√≠nh to√°n)
   ‚Üí C·∫ßn bao nhi√™u ph√©p t√≠nh?

2Ô∏è‚É£ PARALLELIZATION (Kh·∫£ nƒÉng song song h√≥a)
   ‚Üí Bao nhi√™u ph√©p t√≠nh ph·∫£i l√†m tu·∫ßn t·ª±?

3Ô∏è‚É£ PATH LENGTH (ƒê·ªô d√†i ƒë∆∞·ªùng ƒëi)
   ‚Üí Th√¥ng tin c·∫ßn ƒëi qua bao nhi√™u b∆∞·ªõc ƒë·ªÉ k·∫øt n·ªëi 2 t·ª´ xa nhau?


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
5.2. B·∫¢NG SO S√ÅNH CHI TI·∫æT:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

K√Ω hi·ªáu:
‚Ä¢ n = ƒë·ªô d√†i sequence (s·ªë t·ª´ trong c√¢u)
‚Ä¢ d = dimension c·ªßa representation (512)
‚Ä¢ k = kernel width (cho convolutional)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer Type           ‚îÇ Complexity  ‚îÇ Sequential Ops ‚îÇ Max Path     ‚îÇ
‚îÇ                      ‚îÇ per Layer   ‚îÇ                ‚îÇ Length       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Self-Attention       ‚îÇ O(n¬≤¬∑d)     ‚îÇ O(1)          ‚îÇ O(1)         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Recurrent            ‚îÇ O(n¬∑d¬≤)     ‚îÇ O(n)          ‚îÇ O(n)         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Convolutional        ‚îÇ O(k¬∑n¬∑d¬≤)   ‚îÇ O(1)          ‚îÇ O(log‚Çñ(n))   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Self-Attention       ‚îÇ O(r¬∑n¬∑d)    ‚îÇ O(1)          ‚îÇ O(n/r)       ‚îÇ
‚îÇ (restricted)         ‚îÇ             ‚îÇ                ‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
5.3. PH√ÇN T√çCH CHI TI·∫æT T·ª™NG LO·∫†I:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
1Ô∏è‚É£ SELF-ATTENTION LAYER
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Complexity: O(n¬≤¬∑d)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
T√≠nh attention gi·ªØa m·ªçi c·∫∑p t·ª´: n √ó n = n¬≤ c·∫∑p
M·ªói ph√©p t√≠nh c√≥ dimension d
‚Üí T·ªïng: n¬≤ √ó d

V√ç D·ª§: C√¢u 10 t·ª´, dimension 512
n¬≤ √ó d = 10¬≤ √ó 512 = 51,200 ph√©p t√≠nh

Sequential Operations: O(1)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
T·∫§T C·∫¢ c√°c attention scores c√≥ th·ªÉ t√≠nh SONG SONG!
‚Üí Kh√¥ng c·∫ßn ch·ªù ƒë·ª£i g√¨ c·∫£
‚Üí Ch·ªâ 1 b∆∞·ªõc!

Max Path Length: O(1)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
B·∫•t k·ª≥ 2 t·ª´ n√†o c≈©ng k·∫øt n·ªëi TR·ª∞C TI·∫æP qua attention
‚Üí Th√¥ng tin ƒëi ngay 1 b∆∞·ªõc!

V√ç D·ª§: "The cat sitting on the mat is cute"
T·ª´ "cat" ‚Üî "cute": 1 b∆∞·ªõc (tr·ª±c ti·∫øp)
Kh√¥ng quan t√¢m ch√∫ng c√°ch nhau bao nhi√™u t·ª´!


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
2Ô∏è‚É£ RECURRENT LAYER (RNN/LSTM/GRU)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Complexity: O(n¬∑d¬≤)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
X·ª≠ l√Ω n t·ª´
M·ªói t·ª´: matrix multiplication d√ód
‚Üí T·ªïng: n √ó d¬≤

V√ç D·ª§: C√¢u 10 t·ª´, dimension 512
n √ó d¬≤ = 10 √ó 512¬≤ = 2,621,440 ph√©p t√≠nh
‚Üí Nhi·ªÅu h∆°n self-attention g·∫ßn 50 l·∫ßn!

Sequential Operations: O(n)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Ph·∫£i x·ª≠ l√Ω TU·∫¶N T·ª∞ t·ª´ng t·ª´:
t‚ÇÅ ‚Üí t‚ÇÇ ‚Üí t‚ÇÉ ‚Üí ... ‚Üí t‚Çô

‚è±Ô∏è Kh√¥ng th·ªÉ song song h√≥a!
Ph·∫£i ƒë·ª£i xong t‚ÇÅ m·ªõi t√≠nh t‚ÇÇ

Max Path Length: O(n)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Th√¥ng tin ph·∫£i "truy·ªÅn" qua t·∫•t c·∫£ c√°c b∆∞·ªõc

V√ç D·ª§: "The cat sitting on the mat is cute" (8 t·ª´)
T·ª´ "cat" (v·ªã tr√≠ 2) ‚Üí "cute" (v·ªã tr√≠ 8)
‚Üí Ph·∫£i ƒëi qua 6 b∆∞·ªõc!

H·∫≠u qu·∫£: V·ªõi c√¢u d√†i, th√¥ng tin ƒë·∫ßu c√¢u b·ªã "m·ªù ƒëi"


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
3Ô∏è‚É£ CONVOLUTIONAL LAYER
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Complexity: O(k¬∑n¬∑d¬≤)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
k = kernel width (VD: 3, 5, 7...)
n positions, m·ªói position: k √ó d √ó d

V√ç D·ª§: C√¢u 10 t·ª´, kernel 3, dimension 512
k √ó n √ó d¬≤ = 3 √ó 10 √ó 512¬≤ = 7,864,320
‚Üí T·ªën k√©m nh·∫•t!

Sequential Operations: O(1)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
C√≥ th·ªÉ song song h√≥a!
T·∫•t c·∫£ convolutions t√≠nh c√πng l√∫c

Max Path Length: O(log‚Çñ(n))
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
V·ªõi dilated convolutions

V√ç D·ª§: Kernel width k=3, c√¢u 27 t·ª´
log‚ÇÉ(27) = 3 layers c·∫ßn ƒë·ªÉ k·∫øt n·ªëi 2 ƒë·∫ßu c√¢u

T·ªët h∆°n RNN nh∆∞ng v·∫´n t·ªá h∆°n Self-Attention!


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
4Ô∏è‚É£ SELF-ATTENTION (RESTRICTED)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Complexity: O(r¬∑n¬∑d)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
r = neighborhood size (s·ªë t·ª´ xung quanh ƒë∆∞·ª£c xem x√©t)
Ch·ªâ nh√¨n r t·ª´ g·∫ßn nh·∫•t, kh√¥ng nh√¨n to√†n b·ªô

V√ç D·ª§: r=5, ch·ªâ nh√¨n 5 t·ª´ xung quanh
‚Üí Gi·∫£m n¬≤ xu·ªëng r¬∑n

Max Path Length: O(n/r)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
C·∫ßn nhi·ªÅu layers ƒë·ªÉ th√¥ng tin ƒëi xa


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
5.4. K·∫æT LU·∫¨N: T·∫†I SAO CH·ªåN SELF-ATTENTION?
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚úÖ ∆ØU ƒêI·ªÇM 1: NHANH v·ªõi c√¢u th√¥ng th∆∞·ªùng
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Khi n < d (th∆∞·ªùng l√† n < 512):
‚Ä¢ n¬≤¬∑d < n¬∑d¬≤
‚Ä¢ Self-Attention NHANH H∆†N Recurrent!

V√ç D·ª§:
C√¢u 50 t·ª´ (n=50), d=512
‚Ä¢ Self-Attention: 50¬≤ √ó 512 = 1,280,000
‚Ä¢ Recurrent: 50 √ó 512¬≤ = 13,107,200
‚Üí Self-Attention nhanh h∆°n 10 l·∫ßn!


‚úÖ ∆ØU ƒêI·ªÇM 2: SONG SONG H√ìA HO√ÄN TO√ÄN
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Sequential Operations = O(1)
‚Üí T·∫≠n d·ª•ng 100% s·ª©c m·∫°nh GPU!
‚Üí Training nhanh h∆°n R√ÅT NHI·ªÄU


‚úÖ ∆ØU ƒêI·ªÇM 3: H·ªåC LONG-RANGE DEPENDENCIES T·ªêT NH·∫§T
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Max Path Length = O(1)
‚Üí M·ªçi t·ª´ k·∫øt n·ªëi tr·ª±c ti·∫øp!
‚Üí Kh√¥ng b·ªã "qu√™n" th√¥ng tin ƒë·∫ßu c√¢u

V√ç D·ª§ th·ª±c t·∫ø:
C√¢u: "The keys, which I had placed on the kitchen table yesterday 
      morning before leaving for work, are missing."
      
T·ª´ "keys" v√† "are" c√°ch nhau 13 t·ª´
‚Ä¢ RNN: th√¥ng tin ƒëi qua 13 b∆∞·ªõc ‚Üí b·ªã m·ªù
‚Ä¢ Self-Attention: k·∫øt n·ªëi tr·ª±c ti·∫øp ‚Üí r√µ r√†ng!


‚úÖ ∆ØU ƒêI·ªÇM 4: D·ªÑ HI·ªÇU (INTERPRETABLE)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
C√≥ th·ªÉ visualize attention weights
‚Üí Th·∫•y ƒë∆∞·ª£c model "nh√¨n" v√†o ƒë√¢u!

V√ç D·ª§:
Khi d·ªãch "it" trong "The animal didn't cross the street because it was tired"
‚Üí Xem attention weights ƒë·ªÉ bi·∫øt "it" refer t·ªõi "animal" hay "street"


‚ö†Ô∏è NH∆Ø·ª¢C ƒêI·ªÇM: V·ªõi c√¢u R·∫§T d√†i (n >> d)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
n¬≤ c√≥ th·ªÉ r·∫•t l·ªõn!

GI·∫¢I PH√ÅP:
‚Ä¢ Restricted Self-Attention (ch·ªâ nh√¨n r t·ª´ g·∫ßn nh·∫•t)
‚Ä¢ Sparse Attention (c√°c nghi√™n c·ª©u sau n√†y)
‚Ä¢ Sliding Window Attention
‚Ä¢ Longformer, BigBird, etc. (models sau n√†y)

=====================================================================================

**PH·∫¶N 6: QU√Å TR√åNH HU·∫§N LUY·ªÜN (TRAINING)**

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
6.1. D·ªÆ LI·ªÜU V√Ä BATCHING
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
D·ªÆ LI·ªÜU TRAINING:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìä WMT 2014 English-German:
   ‚Ä¢ ~4.5 tri·ªáu c·∫∑p c√¢u
   ‚Ä¢ Encode b·∫±ng Byte-Pair Encoding (BPE)
   ‚Ä¢ Vocabulary chung cho source v√† target: ~37,000 tokens
   
   V√ç D·ª§ BPE:
   "unhappiness" ‚Üí ["un", "happiness"] ‚Üí ["un", "happi", "ness"]
   ‚Üí X·ª≠ l√Ω t·ªët c√°c t·ª´ hi·∫øm!

üìä WMT 2014 English-French:
   ‚Ä¢ ~36 tri·ªáu c√¢u (G·∫§P 8 L·∫¶N!)
   ‚Ä¢ Word-piece vocabulary: 32,000 tokens


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
BATCHING (Gom nh√≥m):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

C√°c c√¢u ƒë∆∞·ª£c gom theo ƒë·ªô d√†i t∆∞∆°ng t·ª± nhau
‚Üí T·ªëi ∆∞u h√≥a memory v√† t·ªëc ƒë·ªô

M·ªói batch ch·ª©a:
‚Ä¢ ~25,000 source tokens
‚Ä¢ ~25,000 target tokens

V√ç D·ª§:
Batch 1: C√°c c√¢u 10-15 t·ª´
Batch 2: C√°c c√¢u 15-20 t·ª´
Batch 3: C√°c c√¢u 20-30 t·ª´


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
6.2. PH·∫¶N C·ª®NG V√Ä TH·ªúI GIAN
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üñ•Ô∏è PH·∫¶N C·ª®NG:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
8 √ó NVIDIA P100 GPUs (m·ªói GPU ~16GB RAM)


‚è±Ô∏è TH·ªúI GIAN TRAINING:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Base Model:
‚Ä¢ T·ªïng th·ªùi gian: ~12 gi·ªù
‚Ä¢ T·ªëc ƒë·ªô: ~0.4 gi√¢y/step
‚Ä¢ T·ªïng s·ªë steps: ~100,000 steps

Big Model:
‚Ä¢ T·ªïng th·ªùi gian: 3.5 ng√†y
‚Ä¢ T·ªëc ƒë·ªô: ~1.0 gi√¢y/step  
‚Ä¢ T·ªïng s·ªë steps: ~300,000 steps

üí∞ SO S√ÅNH CHI PH√ç:
‚Üí R·∫∫ h∆°n R·∫§T NHI·ªÄU so v·ªõi c√°c models tr∆∞·ªõc!
‚Üí K·∫øt qu·∫£ l·∫°i T·ªêT H∆†N!


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
6.3. OPTIMIZER (B·ªò T·ªêI ·ª∞U H√ìA)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

S·ª≠ d·ª•ng: ADAM OPTIMIZER

Tham s·ªë:
‚Ä¢ Œ≤‚ÇÅ = 0.9
‚Ä¢ Œ≤‚ÇÇ = 0.98
‚Ä¢ Œµ = 10‚Åª‚Åπ


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
LEARNING RATE SCHEDULE (Quan tr·ªçng!):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

C√¥ng th·ª©c:
lrate = d_model^(-0.5) ¬∑ min(step^(-0.5), step ¬∑ warmup_steps^(-1.5))

Ho·∫∑c vi·∫øt d·ªÖ hi·ªÉu h∆°n:
lrate = (1/‚àö512) √ó min(1/‚àöstep, step/warmup_steps^1.5)


C√ÅCH HO·∫†T ƒê·ªòNG:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Phase 1: WARMUP (Steps 0 ‚Üí 4000)                 ‚îÇ
‚îÇ Learning rate TƒÇNG d·∫ßn t·ª´ 0 ‚Üí max                ‚îÇ
‚îÇ                                                   ‚îÇ
‚îÇ     /                                             ‚îÇ
‚îÇ    /                                              ‚îÇ
‚îÇ   /                                               ‚îÇ
‚îÇ  /                                                ‚îÇ
‚îÇ /___________________                              ‚îÇ
‚îÇ 0      4000                                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Phase 2: DECAY (Steps 4000 ‚Üí 300,000)            ‚îÇ
‚îÇ Learning rate GI·∫¢M d·∫ßn theo 1/‚àöstep              ‚îÇ
‚îÇ                                                   ‚îÇ
‚îÇ     \                                             ‚îÇ
‚îÇ      \                                            ‚îÇ
‚îÇ       \                                           ‚îÇ
‚îÇ        \______                                    ‚îÇ
‚îÇ         \_____\_____                              ‚îÇ
‚îÇ         4000        300000                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Warmup steps = 4000

T·∫†I SAO C·∫¶N WARMUP?
‚Ä¢ ƒê·∫ßu training, weights random ‚Üí gradient kh√¥ng ·ªïn ƒë·ªãnh
‚Ä¢ Learning rate nh·ªè gi√∫p model "kh·ªüi ƒë·ªông" m∆∞·ª£t m√†
‚Ä¢ Tr√°nh model b·ªã "s·ªëc" v√† diverge (ph√¢n k·ª≥)


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
6.4. REGULARIZATION (CH·ªêNG OVERFITTING)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

√Åp d·ª•ng 3 k·ªπ thu·∫≠t regularization:

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
1Ô∏è‚É£ RESIDUAL DROPOUT
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

√Åp d·ª•ng dropout ·ªü 2 v·ªã tr√≠:

V·ªã tr√≠ 1: Sau m·ªói sub-layer
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Sub-layer output                     ‚îÇ
‚îÇ         ‚Üì                             ‚îÇ
‚îÇ Dropout (P_drop = 0.1)               ‚îÇ
‚îÇ         ‚Üì                             ‚îÇ
‚îÇ + Residual connection                ‚îÇ
‚îÇ         ‚Üì                             ‚îÇ
‚îÇ Layer Normalization                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

V·ªã tr√≠ 2: Sau embedding + positional encoding
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Word Embedding + Positional Encoding ‚îÇ
‚îÇ         ‚Üì                             ‚îÇ
‚îÇ Dropout (P_drop = 0.1)               ‚îÇ
‚îÇ         ‚Üì                             ‚îÇ
‚îÇ V√†o Encoder/Decoder                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

P_drop = 0.1 ‚Üí Randomly "t·∫Øt" 10% neurons


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
2Ô∏è‚É£ LABEL SMOOTHING
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Œµ_ls = 0.1

KH√îNG S·ª¨ D·ª§NG LABEL SMOOTHING:
Target: "cat"
Label: [0, 0, 0, 1, 0, 0, ...] (1 cho "cat", 0 cho c√°c t·ª´ kh√°c)
       ‚Üí Model qu√° "t·ª± tin" 100%

S·ª¨ D·ª§NG LABEL SMOOTHING:
Target: "cat"
Label: [0.01, 0.01, 0.01, 0.91, 0.01, 0.01, ...]
       ‚Üí Ph√¢n m·ªôt ch√∫t x√°c su·∫•t cho c√°c t·ª´ kh√°c
       ‚Üí Model kh√¥ng qu√° t·ª± tin

K·∫æT QU·∫¢:
‚Ä¢ Perplexity tƒÉng l√™n (model uncertain h∆°n)
‚Ä¢ NH∆ØNG accuracy v√† BLEU score T·ªêT H∆†N!
‚Ä¢ Model generalize t·ªët h∆°n (kh√¥ng overfitting)

V√ç D·ª§:
Khi d·ªãch "good" ‚Üí "t·ªët"
Kh√¥ng n√≥i 100% l√† "t·ªët", m√† 90% "t·ªët", 5% "hay", 5% "gi·ªèi"...
‚Üí H·ªçc ƒë∆∞·ª£c c√°c t·ª´ ƒë·ªìng nghƒ©a!


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
3Ô∏è‚É£ CHECKPOINT AVERAGING
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

L∆∞u model m·ªói 10 ph√∫t
‚Üí L·∫•y 8 checkpoints cu·ªëi c√πng
‚Üí Average weights c·ªßa ch√∫ng
‚Üí Model ·ªïn ƒë·ªãnh v√† t·ªët h∆°n!

=====================================================================================

**PH·∫¶N 7: K·∫æT QU·∫¢ (RESULTS)**

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
7.1. K·∫æT QU·∫¢ D·ªäCH M√ÅY (MACHINE TRANSLATION)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üìä BLEU SCORE l√† g√¨?
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
BLEU (Bilingual Evaluation Understudy)
‚Üí Ch·ªâ s·ªë ƒëo ch·∫•t l∆∞·ª£ng d·ªãch m√°y
‚Üí So s√°nh b·∫£n d·ªãch m√°y v·ªõi b·∫£n d·ªãch c·ªßa ng∆∞·ªùi
‚Üí C√†ng cao c√†ng t·ªët (max = 100, th·ª±c t·∫ø ~40-50 l√† r·∫•t t·ªët)


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üá©üá™ WMT 2014 ENGLISH ‚Üí GERMAN
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Transformer (big): BLEU = 28.4
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚ú® K·ª∂ L·ª§C M·ªöI!
‚ú® T·ªët h∆°n 2.0 BLEU so v·ªõi record tr∆∞·ªõc ƒë√≥
‚ú® Chi ph√≠ training: CH·ªà 1/4 so v·ªõi models tr∆∞·ªõc!

So s√°nh:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Model                      ‚îÇ BLEU     ‚îÇ Training   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Previous Best (ensemble)   ‚îÇ 26.4     ‚îÇ 14+ ng√†y   ‚îÇ
‚îÇ Transformer (base)         ‚îÇ 27.3     ‚îÇ 12 gi·ªù     ‚îÇ
‚îÇ Transformer (big)          ‚îÇ 28.4     ‚îÇ 3.5 ng√†y   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üá´üá∑ WMT 2014 ENGLISH ‚Üí FRENCH
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Transformer (big): BLEU = 41.0
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚ú® K·ª∂ L·ª§C M·ªöI cho single model!
‚ú® Chi ph√≠ < 1/4 so v·ªõi model t·ªët nh·∫•t tr∆∞·ªõc ƒë√≥

V·ªõi Ensemble (k·∫øt h·ª£p 8 models):
‚Üí BLEU = 41.8 (C·ª∞C K·ª≤ ·∫§N T∆Ø·ª¢NG!)


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üèÜ ENSEMBLE TECHNIQUE:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. L∆∞u 8 checkpoints cu·ªëi (m·ªói 10 ph√∫t)
2. Average predictions c·ªßa t·∫•t c·∫£ 8 models
3. S·ª≠ d·ª•ng beam search v·ªõi length penalty Œ±=0.6

K·∫øt qu·∫£:
‚Ä¢ English-German: 28.4 ‚Üí 30.2 BLEU
‚Ä¢ English-French: 41.0 ‚Üí 41.8 BLEU


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
7.2. PH√ÇN T√çCH C√ÅC BI·∫æN TH·ªÇ M√î H√åNH (MODEL VARIATIONS)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

ABLATION STUDY = B·ªè/thay ƒë·ªïi t·ª´ng ph·∫ßn ƒë·ªÉ xem ph·∫ßn n√†o quan tr·ªçng

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üî¨ TH·ª∞C NGHI·ªÜM 1: S·ªë l∆∞·ª£ng Attention Heads
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ S·ªë heads   ‚îÇ BLEU     ‚îÇ Nh·∫≠n x√©t            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1          ‚îÇ 24.9     ‚îÇ ‚ùå Gi·∫£m 0.9 BLEU    ‚îÇ
‚îÇ 4          ‚îÇ 25.5     ‚îÇ ‚ö†Ô∏è Ch∆∞a t·ªëi ∆∞u      ‚îÇ
‚îÇ 8 ‚≠ê       ‚îÇ 25.8     ‚îÇ ‚úÖ T·ªêT NH·∫§T         ‚îÇ
‚îÇ 16         ‚îÇ 25.4     ‚îÇ ‚ö†Ô∏è Gi·∫£m nh·∫π         ‚îÇ
‚îÇ 32         ‚îÇ 25.0     ‚îÇ ‚ùå Qu√° nhi·ªÅu heads  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

K·∫æT LU·∫¨N: h = 8 l√† optimal cho d_model = 512


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üî¨ TH·ª∞C NGHI·ªÜM 2: Attention Key Size (d_k)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ d_k    ‚îÇ BLEU     ‚îÇ Nh·∫≠n x√©t                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 32     ‚îÇ 24.5     ‚îÇ ‚ùå Qu√° nh·ªè, hurt quality    ‚îÇ
‚îÇ 64 ‚≠ê  ‚îÇ 25.8     ‚îÇ ‚úÖ T·ªêT NH·∫§T                 ‚îÇ
‚îÇ 128    ‚îÇ 25.7     ‚îÇ ‚ö†Ô∏è T∆∞∆°ng t·ª±, t·ªën RAM h∆°n   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

K·∫æT LU·∫¨N: d_k qu√° nh·ªè l√†m gi·∫£m kh·∫£ nƒÉng bi·ªÉu di·ªÖn


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üî¨ TH·ª∞C NGHI·ªÜM 3: K√≠ch th∆∞·ªõc Model
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ d_model    ‚îÇ d_ff  ‚îÇ BLEU     ‚îÇ Nh·∫≠n x√©t    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 256        ‚îÇ 1024  ‚îÇ 23.7     ‚îÇ ‚ùå Qu√° nh·ªè  ‚îÇ
‚îÇ 512 ‚≠ê     ‚îÇ 2048  ‚îÇ 25.8     ‚îÇ ‚úÖ Base     ‚îÇ
‚îÇ 1024       ‚îÇ 4096  ‚îÇ 26.4     ‚îÇ ‚úÖ Big      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

K·∫æT LU·∫¨N: 
‚Ä¢ Model l·ªõn h∆°n ‚Üí k·∫øt qu·∫£ t·ªët h∆°n
‚Ä¢ Dropout r·∫•t quan tr·ªçng ƒë·ªÉ tr√°nh overfitting


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üî¨ TH·ª∞C NGHI·ªÜM 4: Positional Encoding Type
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Type                 ‚îÇ BLEU     ‚îÇ Nh·∫≠n x√©t    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Sinusoidal ‚≠ê        ‚îÇ 25.8     ‚îÇ ‚úÖ Fixed    ‚îÇ
‚îÇ Learned              ‚îÇ 25.7     ‚îÇ ‚âà T∆∞∆°ng t·ª±  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

K·∫æT LU·∫¨N: 
‚Ä¢ C·∫£ 2 c√°ch ƒë·ªÅu t·ªët!
‚Ä¢ Ch·ªçn Sinusoidal v√¨ kh·∫£ nƒÉng extrapolate


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
7.3. ENGLISH CONSTITUENCY PARSING
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üí° ƒê√ÇY L√Ä TH·ª∞C NGHI·ªÜM QUAN TR·ªåNG!
‚Üí Ch·ª©ng minh Transformer kh√¥ng ch·ªâ t·ªët cho d·ªãch m√°y
‚Üí M√† c√≥ th·ªÉ GENERALIZE sang c√°c tasks kh√°c!


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
CONSTITUENCY PARSING L√Ä G√å?
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Ph√¢n t√≠ch c·∫•u tr√∫c ng·ªØ ph√°p c·ªßa c√¢u

V√ç D·ª§:
Input: "The cat sleeps"

Output (parse tree):
                S
               / \
              NP  VP
             / \   |
           Det  N  V
            |   |  |
          The cat sleeps

‚Üí X√°c ƒë·ªãnh ch·ªß ng·ªØ, ƒë·ªông t·ª´, c·ª•m danh t·ª´, c·ª•m ƒë·ªông t·ª´...


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
SETUP:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Dataset: Penn Treebank (Wall Street Journal)
‚Ä¢ Training: ~40,000 c√¢u

Model: Transformer (4 layers, d_model=1024)
‚Ä¢ Kh√¥ng c√≥ task-specific tuning!
‚Ä¢ Ch·ªâ thay ƒë·ªïi k√≠ch th∆∞·ªõc model


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
K·∫æT QU·∫¢:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Model                          ‚îÇ F1     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Previous Best Models           ‚îÇ ~91.0  ‚îÇ
‚îÇ Transformer (WSJ only) ‚≠ê      ‚îÇ 91.3   ‚îÇ
‚îÇ Transformer (+ BerkleyParser)  ‚îÇ 92.7   ‚îÇ
‚îÇ RNN Grammar (state-of-art)     ‚îÇ 93.3   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üéâ K·∫æT QU·∫¢ ·∫§N T∆Ø·ª¢NG:
‚Ä¢ ƒê·ª©ng th·ª© 2, ch·ªâ sau RNN Grammar
‚Ä¢ NH∆ØNG Transformer KH√îNG c√≥ task-specific tuning
‚Ä¢ Ch·ª©ng minh t√≠nh GENERAL c·ªßa architecture!

=====================================================================================

**PH·∫¶N 8: K·∫æT LU·∫¨N V√Ä √ù NGHƒ®A L·ªäCH S·ª¨**

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
8.1. T√ìM T·∫ÆT TRANSFORMER
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üèÜ TRANSFORMER l√† m√¥ h√¨nh sequence transduction ƒê·∫¶U TI√äN:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚Ä¢ D·ª±a HO√ÄN TO√ÄN tr√™n attention
‚Ä¢ KH√îNG d√πng recurrence (RNN/LSTM/GRU)
‚Ä¢ KH√îNG d√πng convolution (CNN)
‚Ä¢ Thay th·∫ø b·∫±ng Multi-Headed Self-Attention


‚úÖ ∆ØU ƒêI·ªÇM V∆Ø·ª¢T TR·ªòI:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. T·ªêC ƒê·ªò TRAINING:
   ‚Ä¢ Nhanh h∆°n NHI·ªÄU L·∫¶N so v·ªõi RNN/CNN-based models
   ‚Ä¢ Song song h√≥a ho√†n to√†n tr√™n GPU
   ‚Ä¢ Base model: 12 gi·ªù vs nhi·ªÅu ng√†y c·ªßa models kh√°c

2. CH·∫§T L∆Ø·ª¢NG:
   ‚Ä¢ English-German: 28.4 BLEU (improve 2.0 BLEU)
   ‚Ä¢ English-French: 41.8 BLEU (state-of-the-art)
   ‚Ä¢ Constituency Parsing: F1 = 92.7

3. HI·ªÜU QU·∫¢:
   ‚Ä¢ Chi ph√≠ training < 1/4 so v·ªõi models tr∆∞·ªõc
   ‚Ä¢ K·∫øt qu·∫£ t·ªët h∆°n v·ªõi chi ph√≠ th·∫•p h∆°n

4. T√çNH T·ªîNG QU√ÅT:
   ‚Ä¢ Kh√¥ng ch·ªâ t·ªët cho machine translation
   ‚Ä¢ M√† c√≤n t·ªët cho parsing v√† c√°c NLP tasks kh√°c


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
8.2. H∆Ø·ªöNG PH√ÅT TRI·ªÇN T∆Ø∆†NG LAI (T·ª´ paper)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üìã C√ÅC K·∫æ HO·∫†CH:

1Ô∏è‚É£ √Åp d·ª•ng cho problems kh√°c ngo√†i text:
   ‚Ä¢ Images (Computer Vision)
   ‚Ä¢ Audio (Speech Recognition)
   ‚Ä¢ Video (Video Understanding)

2Ô∏è‚É£ Nghi√™n c·ª©u local, restricted attention:
   ‚Ä¢ ƒê·ªÉ x·ª≠ l√Ω hi·ªáu qu·∫£ large inputs/outputs
   ‚Ä¢ Gi·∫£m complexity t·ª´ O(n¬≤) xu·ªëng O(n)

3Ô∏è‚É£ Making generation less sequential:
   ‚Ä¢ TƒÉng t·ªëc ƒë·ªô inference
   ‚Ä¢ Parallel decoding


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
8.3. T·∫¶M QUAN TR·ªåNG L·ªäCH S·ª¨ (Nh√¨n l·∫°i t·ª´ 2026)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üåü TRANSFORMER ƒê√É THAY ƒê·ªîI HO√ÄN TO√ÄN NLP V√Ä AI:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Paper n√†y (2017) ƒë√£ m·ªü ra m·ªôt k·ª∑ nguy√™n m·ªõi!


üìà D√íNG TH·ªúI GIAN:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

2017: Transformer ra ƒë·ªùi
       ‚Üì
2018: BERT (Google) - Bidirectional Encoder
      GPT-1 (OpenAI) - Decoder-only
       ‚Üì
2019: GPT-2, RoBERTa, XLNet, ALBERT, T5
       ‚Üì
2020: GPT-3 (175B parameters!)
       ‚Üì
2021: Switch Transformer (1.6T parameters)
      DALL-E (text ‚Üí image)
       ‚Üì
2022: ChatGPT (GPT-3.5)
      Stable Diffusion
       ‚Üì
2023: GPT-4, LLaMA, Claude, Gemini
      Multi-modal models
       ‚Üì
2024-2026: Ti·∫øp t·ª•c ph√°t tri·ªÉn...


üèóÔ∏è C√ÅC M√î H√åNH D·ª∞A TR√äN TRANSFORMER:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ENCODER-ONLY (for understanding):                   ‚îÇ
‚îÇ ‚Ä¢ BERT, RoBERTa, ALBERT, ELECTRA                   ‚îÇ
‚îÇ ‚Ä¢ D√πng cho: classification, NER, QA...             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DECODER-ONLY (for generation):                      ‚îÇ
‚îÇ ‚Ä¢ GPT-1/2/3/4, LLaMA, Mistral, Gemini              ‚îÇ
‚îÇ ‚Ä¢ D√πng cho: text generation, chatbots...           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ENCODER-DECODER (for seq2seq):                      ‚îÇ
‚îÇ ‚Ä¢ T5, BART, mT5, mBART                             ‚îÇ
‚îÇ ‚Ä¢ D√πng cho: translation, summarization...          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MULTI-MODAL:                                        ‚îÇ
‚îÇ ‚Ä¢ CLIP, DALL-E (text + image)                      ‚îÇ
‚îÇ ‚Ä¢ Whisper (audio)                                   ‚îÇ
‚îÇ ‚Ä¢ GPT-4V (text + image + audio)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


üí° NH·ªÆNG ƒê√ìNG G√ìP THEN CH·ªêT C·ª¶A TRANSFORMER:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1Ô∏è‚É£ CH·ª®NG MINH: Attention mechanism l√† ƒê·ª¶!
   ‚Ä¢ Kh√¥ng c·∫ßn RNN
   ‚Ä¢ Kh√¥ng c·∫ßn CNN
   ‚Ä¢ Ch·ªâ c·∫ßn Attention!

2Ô∏è‚É£ SONG SONG H√ìA HO√ÄN TO√ÄN:
   ‚Ä¢ T·∫≠n d·ª•ng ƒë∆∞·ª£c s·ª©c m·∫°nh GPU
   ‚Ä¢ Training nhanh h∆°n nhi·ªÅu l·∫ßn
   ‚Ä¢ M·ªü ƒë∆∞·ªùng cho Large Language Models

3Ô∏è‚É£ SCALABILITY:
   ‚Ä¢ Ki·∫øn tr√∫c scale t·ªët
   ‚Ä¢ T·ª´ 65M parameters (base) ‚Üí h√†ng trƒÉm t·ª∑ parameters
   ‚Ä¢ C√†ng l·ªõn c√†ng t·ªët (v·ªõi ƒë·ªß d·ªØ li·ªáu)

4Ô∏è‚É£ TRANSFER LEARNING:
   ‚Ä¢ Pre-train 1 l·∫ßn
   ‚Ä¢ Fine-tune cho nhi·ªÅu tasks
   ‚Ä¢ C∆° s·ªü cho BERT, GPT...

5Ô∏è‚É£ INTERPRETABILITY:
   ‚Ä¢ C√≥ th·ªÉ visualize attention
   ‚Ä¢ Hi·ªÉu ƒë∆∞·ª£c model "nghƒ©" g√¨
   ‚Ä¢ Quan tr·ªçng cho research v√† debugging


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
8.4. M√É NGU·ªíN
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üîó Code (TensorFlow):
https://github.com/tensorflow/tensor2tensor

üîó PyTorch implementations (community):
‚Ä¢ https://github.com/harvardnlp/annotated-transformer
‚Ä¢ https://github.com/jadore801120/attention-is-all-you-need-pytorch


=====================================================================================

**PH·∫¶N 9: C√ÅC KH√ÅI NI·ªÜM QUAN TR·ªåNG - √îN T·∫¨P**

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
THU·∫¨T NG·ªÆ CH√çNH:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üìö EMBEDDING:
   Chuy·ªÉn t·ª´/token th√†nh vector s·ªë
   VD: "m√®o" ‚Üí [0.2, 0.8, 0.1, ...]

üìö ATTENTION:
   C∆° ch·∫ø gi√∫p model "ch√∫ √Ω" v√†o ƒë√¢u
   T√≠nh to√°n m·ªëi quan h·ªá gi·ªØa c√°c t·ª´

üìö QUERY, KEY, VALUE:
   ‚Ä¢ Query: ƒêi·ªÅu ƒëang t√¨m ki·∫øm
   ‚Ä¢ Key: Ti√™u ƒë·ªÅ/nh√£n
   ‚Ä¢ Value: N·ªôi dung th·ª±c s·ª±

üìö SELF-ATTENTION:
   Attention trong c√πng 1 sequence
   C√°c t·ª´ "nh√¨n th·∫•y" nhau

üìö MULTI-HEAD ATTENTION:
   Nhi·ªÅu attention song song
   M·ªói head h·ªçc 1 ki·ªÉu m·ªëi quan h·ªá

üìö POSITIONAL ENCODING:
   Th√™m th√¥ng tin v·ªã tr√≠ v√†o embedding
   D√πng sin/cos functions

üìö RESIDUAL CONNECTION:
   x + F(x) thay v√¨ ch·ªâ F(x)
   Gi√∫p training d·ªÖ h∆°n

üìö LAYER NORMALIZATION:
   Chu·∫©n h√≥a output c·ªßa layer
   ·ªîn ƒë·ªãnh training

üìö FEED-FORWARD NETWORK:
   MLP ƒë∆°n gi·∫£n: Linear ‚Üí ReLU ‚Üí Linear
   √Åp d·ª•ng ri√™ng cho t·ª´ng v·ªã tr√≠

üìö ENCODER:
   X·ª≠ l√Ω input sequence
   6 layers √ó (Self-Attention + FFN)

üìö DECODER:
   T·∫°o output sequence
   6 layers √ó (Masked Self-Att + Cross-Att + FFN)

üìö MASKING:
   Che th√¥ng tin t∆∞∆°ng lai
   Gi·ªØ t√≠nh auto-regressive

üìö BEAM SEARCH:
   T√¨m ki·∫øm output t·ªët nh·∫•t
   Gi·ªØ k candidates t·ªët nh·∫•t

üìö BLEU SCORE:
   ƒêo ch·∫•t l∆∞·ª£ng d·ªãch m√°y
   So s√°nh v·ªõi b·∫£n d·ªãch tham kh·∫£o


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
C√îNG TH·ª®C QUAN TR·ªåNG:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

1Ô∏è‚É£ Scaled Dot-Product Attention:
   Attention(Q,K,V) = softmax(QK·µÄ/‚àöd‚Çñ)V

2Ô∏è‚É£ Multi-Head Attention:
   MultiHead(Q,K,V) = Concat(head‚ÇÅ,...,head‚Çà)W·¥º
   head·µ¢ = Attention(QW·µ¢Q, KW·µ¢K, VW·µ¢V)

3Ô∏è‚É£ Position-wise FFN:
   FFN(x) = max(0, xW‚ÇÅ+b‚ÇÅ)W‚ÇÇ+b‚ÇÇ

4Ô∏è‚É£ Positional Encoding:
   PE(pos,2i) = sin(pos/10000^(2i/d_model))
   PE(pos,2i+1) = cos(pos/10000^(2i/d_model))

5Ô∏è‚É£ Layer with Residual:
   Output = LayerNorm(x + Sublayer(x))


‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
TH√îNG S·ªê M√î H√åNH:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

BASE MODEL:
‚Ä¢ Layers: N = 6 (encoder + decoder)
‚Ä¢ d_model = 512
‚Ä¢ d_ff = 2048
‚Ä¢ h = 8 heads
‚Ä¢ d_k = d_v = 64
‚Ä¢ P_drop = 0.1

BIG MODEL:
‚Ä¢ Layers: N = 6
‚Ä¢ d_model = 1024
‚Ä¢ d_ff = 4096
‚Ä¢ h = 16 heads
‚Ä¢ d_k = d_v = 64
‚Ä¢ P_drop = 0.3